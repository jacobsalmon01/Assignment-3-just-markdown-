## Compression Ratios Table (compressed size, compression ratio)
| File name | File Size | Original LZW | LZWmod -n | LZWmod -r | Unix |
|-----------|-----------|--------------|-----------|-----------|------
| all.tar | 3,031,040 bytes | 1,846,854 bytes, 1.641 | 1,792,781 bytes, 1.691 | 1,177,879 bytes, 2.573 | 969,190 bytes, 3.127 |
| assig2.doc | 87,040 bytes | 74,574 bytes, 1.167 | 40,039 bytes, 2.174 |  40,039 bytes, 2.174 | 23,440 bytes, 3.713 |
| bmps.tar | 1,105,920 bytes | 925,079 bytes, 1.195 | 80,913 bytes, 13.668 | 80,913 bytes, 13.668 | 63,944 bytes, 17.295 |
| code2.txt | 57,701 bytes | 24,138 bytes, 2.390 | 20,515 bytes, 2.813 | 20,515 bytes, 2.813 | 63,944 bytes, 17.295 |
| edit.exe | 236,328 bytes | 250,742 bytes, 0.943 | 156,409 bytes, 1.511 | 152,229 bytes, 1.552 | 126,977 byte, 1.861 |
| frosty.jpg | 126,748 bytes | 177,453 bytes, 0.714 | 163,789 bytes, 0.774 | 171,168 bytes, 0.740 | 127,013 bytes, 0.998 |
| gone_fishing.bmp.Z | 8,964 bytes | 12,702 bytes, 0.706 | 12,568 bytes, 0.713 | 12,598 bytes, 0.713 | 9,308 bytes, 0.963 |
| large.txt | 1,220,703 bytes | 605,184 bytes, 2.017 | 501,777 bytes, 2.433 | 527,594 bytes, 2.314 | 493,407 bytes, 2.474 | 
| Lego-big.gif | 93,371 bytes | 128,973 bytes, 0.724 | 122,493 bytes, 0.762 | 122,493 bytes, 0.762 | 92,804 bytes, 1.006 |
| medium.txt | 25,407 bytes | 13,197 bytes, 1.925 | 12,530 bytes, 2.028 | 12,530 bytes, 2.028 | 11,138 bytes, 2.281 |
| texts.tar | 1,382,400 bytes | 1,012,179 bytes, 1.366 | 597,847 bytes, 2.312 | 590,541 bytes, 2.340 | 533,654 bytes, 2.590 |
| wacky.bmp | 921,654 bytes | 4,302 bytes, 214.238 | 3,951 bytes, 233.271 | 3,951 bytes, 233.271 | 3,276 bytes, 281.335 |
| winnt256.bmp | 157,044 bytes | 159,050 bytes, 0.987 | 62,931 bytes, 2.495 | 62,931 bytes, 2.495 | 50,258 bytes, 3.125 |

## Write-up
### Original LZW Implementation
For every test file that was provided, the original LZW implementation gave the worst compression ratio by far. Not only did the original LZW implementation provide a lower compression ratio, but the overhead and runtime differences between the original implementations and the improved ones was noticable even without runtime analysis. For example, the all.tar file took nearly 15 minutes to compress into all.lzw, compared to the <60 seconds required for the other implementations. Much of this time difference is likely due to the fact that the TST.java file was implemented using a String object, as opposed to a StringBuilder object - this makes a huge difference since appending to strings is such a crucial part of the compression process, and appending with a String object requires the creation of an entirely new String object. On the otherhand, appending to a StringBuilder is a fast process with a constant runtime that does not require overhead. As for the compression ratio itself, the biggest difference maker is certainly the lack of use of variable-length codewords. Fixed-length codewords, as used in the original LZW implementation, use constant-sized blocks of storage to represent strings/characters no matter their frequency in the file, which means that even if a given string is encountered only a single time in a file, it will take the same amount of space as one that could be encountered 1000 times. The issue with this is evident, and it is addressed in the other implementations of the LZW compression. The best compression ratio for the original implementation is found in wacky.bmp, and this is likely due to the very small amount of entropy encountered in the file - the file is about 90% white space, which makes compression easy and efficient since large blocks can be represented. The file with the worst compression ratio for the original implementation is gone_fishing.bmp.Z - I believe this is due to the fact that this file is an image that was already compressed by the Unix compression system, so passing it through the original LZW algorithm (which is inferior) would only make the compression ratio worse.

### Modified LZW Implementation, no reset
In general, the modified LZW implementation without dictionary reset performed worse than or equal to the same implementation *with* reset, but not by a huge margin. There was only two cases where the LZWmod with no reset performed better than that with reset; one of those files is frosty.jpg - I believe this is the case in part because .jpg files are already compressed, since images can take up so much storage space, so this is somewhat of an irregular case (given that we're trying to compress a compressed file). The other case where no reset performed better than reset is large.txt, which really surprised me. I am honestly not sure why this is the case, although I am certainly curious. The many cases where LZW without reset was equivalent to LZW with reset is likely because the file size is relatively small, so the original symbol table did not need to be reset. The best compression ratio for LZWmod without reset was again wacky.bmp, due to the small amount of entropy contained in the image. The worst compression ratio for this implementation was again gone_fishing.bmp.Z, since we are trying to compress a file that was already Unix compressed.

### Modified LZW Implementation, with reset
For the most part, this implementation performed better or equally to the same implementation but *without* dictionary reset, but generally not by a large amount. The files where LZW with reset performed better (not equally to) the LZW without reset were all.tar, edit.exe, and texts.tar. This is honestly less of a difference between the two implementations than I anticipated, but it signifies to me that a dictionary reset feature can definitely come in handy when attempting to compress large files such as all.tar, where the reset feature made the compression ratio better by a factor of 1.52. This can be quite significant when you are working with many large files, and it makes sense, since the dictionary reset is best utilized once the symbol table/dictionary becomes filled with codewords multiple times, which only happens with larger files. I am still stumped as to why large.txt performed more poorly with this implementation than the latter, consider it is one of the larger file sizes we worked with on this assignment. The best compression ratio for LZWmod with reset was, just like the rest, wacky.bmp, again due to the small amount of entropy encountered in the file. The worst compression ratio for LZWmod with reset was, as expected, gone_fishing.bmp.Z, since we are attempting to compress a file that was already compressed with a superior compression algorithm.

### Unix Implementation
It did not come as a surprise to me at all to see that the Unix compression algorithm beat out every other implementation that we considered, given that it is the compression algorithm built into Mac and Linux machines. These are tech companies with the capabilities analyze various different implementations, and to run extensive trial-and-error tests to determine the best one. I am not sure if Unix compression is **the** most efficient compression algorithm out there, but it certainly beat out all the other implementations, and often by a large margin. For example, code2.txt had a compression ratio of 2.813 for our LZWmod implementations, but using Unix, a compression ratio of 17.295 is derived - this is a factor of 6.15, which is **extremely** significant when compressing either large files or a large amount of files. As expected, the best compression ratio with Unix compression was in wacky.bmp, and the worst compression ratio was in gone_fishing.bmp.Z (although the compression ratio was very close to 1:1, at 0.998 - likely because we were compressing a file with Unix, that was already compressed by Unix).
